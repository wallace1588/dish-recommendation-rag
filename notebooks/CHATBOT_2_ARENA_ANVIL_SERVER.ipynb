{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyQ8wTx61shQ3XyK5BhYnz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wallace1588/tcc-dish-recommendation-rag/blob/main/notebooks/CHATBOT_2_ARENA_ANVIL_SERVER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3yPJNt1MXKJ"
      },
      "outputs": [],
      "source": [
        "BATTLE_NAME = 'BATTLE_NUM'\n",
        "UP_LINK = 'ANVIL_LINK'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxjqqegqyye6"
      },
      "outputs": [],
      "source": [
        "!CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install pymongo anvil-uplink sentence-transformers langchain-community bitsandbytes flash_attn accelerate llama-cpp-python==0.2.88 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
        "!curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install.sh | bash -s -- -p /usr/local --plugins wasi_logging wasi_nn-ggml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctfLow-uxChG"
      },
      "source": [
        "Configuração para cpu: comente o import llama, a quantização, o tokenizer cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQD6KGWbo6rv"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import MongoDBAtlasVectorSearch\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from pymongo.mongo_client import MongoClient\n",
        "from huggingface_hub import hf_hub_download\n",
        "from torch import bfloat16\n",
        "import torch\n",
        "import transformers\n",
        "import urllib3\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import userdata\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from llama_cpp import Llama\n",
        "from multiprocessing import Process\n",
        "import csv\n",
        "import anvil.server\n",
        "\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QfglwB7rXLe"
      },
      "outputs": [],
      "source": [
        "N_TOKENS = 200\n",
        "N_GPU_LAYERS = 120\n",
        "\n",
        "class standard_model_eval:\n",
        "  def __init__(self, model_id, retriever, flag_chat = 0):\n",
        "    self.model_id = model_id\n",
        "    self.flag_chat = flag_chat\n",
        "    self.stop_strings =  [\"user:\", \"Recbot:\"]\n",
        "    self.retriever = retriever\n",
        "    self.tokenizer = None\n",
        "    self.model = None\n",
        "\n",
        "  def create(self):\n",
        "    bnb_config = transformers.BitsAndBytesConfig(load_in_4bit = True, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=bfloat16)\n",
        "    self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_id, token=HF_TOKEN)\n",
        "    self.model = transformers.AutoModelForCausalLM.from_pretrained(self.model_id, quantization_config=bnb_config, token=HF_TOKEN)\n",
        "\n",
        "  def generate_text(self, prompt):\n",
        "    inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = self.model.generate(inputs, max_new_tokens = N_TOKENS, stop_strings = self.stop_strings, temperature=1.0, tokenizer = self.tokenizer, pad_token_id=self.tokenizer.eos_token_id)\n",
        "    outputs =  outputs[0:,len(inputs[0]):]\n",
        "    output_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return output_text\n",
        "\n",
        "  def get_response(self, input_text):\n",
        "    text = self.generate_text(self.prompt_gen(input_text))\n",
        "    response = ''\n",
        "    for stop_string in self.stop_strings:\n",
        "      if text.find(stop_string) < 0:\n",
        "        continue\n",
        "      else:\n",
        "        response = text[:text.find(stop_string)].strip('\\n')\n",
        "        break\n",
        "    if response == '':\n",
        "      response = text.strip('\\n')\n",
        "    return response\n",
        "\n",
        "  def get_sugestions(self, input_text):\n",
        "    THRESHOLD_MIN = 0.825\n",
        "    list_docs = self.retriever.similarity_search_with_score(input_text, 3)\n",
        "    intro_sugestions = 'Sugestões: '\n",
        "    sugestions = intro_sugestions\n",
        "    for r in list_docs:\n",
        "      dish_description = r[0].page_content\n",
        "      if r[1] > THRESHOLD_MIN:\n",
        "        sugestions = sugestions + dish_description + '; '\n",
        "    if sugestions == intro_sugestions:\n",
        "      sugestions = 'Não há sugestões.'\n",
        "    return sugestions\n",
        "\n",
        "  def prompt_gen(self, user_input_list):\n",
        "    prompt = \"\"\n",
        "    prompt_shots = [\n",
        "      {\"role\":\"system\",\"content\":\"Seu nome é Recbot e você deve fazer recomendações para auxiliar os usuários na escolha de refeições e restaurantes. Use as informações de entrada do usuário para gerar sua resposta. Caso não haja sugestões, você deve solicitar mais informações ao usuário ou responder baseado no histórico da conversa. Suas respostas devem conter apenas um linha.\"},\n",
        "      {\"role\":\"user\",\"content\":\"Estou com fome.\\nNão há sugestões.\"},\n",
        "      {\"role\":\"assistant\",\"content\":\"Estou aqui para te ajudar. O que você gosta de comer?\"},\n",
        "      {\"role\":\"user\",\"content\":\"queria tomar guarana jesus.\\nSugestões: Guarana Jesus: lata 350ml, 7.15, Pasta in Cup, 4.9, https://www.ifood.com.br/delivery/recife-pe/pasta-in-cup-torre/2d4543fd-cb1b-4cb8-b4c8-7c778dfacc0c, ifood; Refrigerante Guarana Jesus 350ml: lata 350ml, 7.0, Donico Coxinhas Artesanais Com Massa de, 4.9, https://www.ifood.com.br/delivery/recife-pe/donico-coxinhas-artesanais-com-massa-de-prado/ce9fffb6-23f4-4117-a604-9ff71ba78405, ifood; Refrigerante Jesus Guarana Lata 350ml: lata 350ml, 4.6, Padaria Ipiranga, 4.9, https://www.ifood.com.br/delivery/recife-pe/padaria-ipiranga-afogados/eb4b66e2-8469-46d5-acf0-b905326fe38f, ifood;\"},\n",
        "      {\"role\":\"assistant\",\"content\":\"Uma boa opção é o Guaraná Jesus 350ml da Padaria Ipiranga, por apenas R$ 4.60. Tamnbém tem no Pasta in Cup e no Donico Coxinhas Artesanais Com Massa de. Para mais informações acesse o link: https://www.ifood.com.br/delivery/recife-pe/padaria-ipiranga-afogados/eb4b66e2-8469-46d5-acf0-b905326fe38f.\"},\n",
        "      {\"role\":\"user\",\"content\":\"Onde posso comprar?\\nNão há sugestões.\"},\n",
        "      {\"role\":\"assistant\",\"content\":\"Você pode comprar direto na Padaria Ipiranga ou através do site: https://www.ifood.com.br/delivery/recife-pe/padaria-ipiranga-afogados/eb4b66e2-8469-46d5-acf0-b905326fe38f.\"},\n",
        "      {\"role\":\"user\",\"content\":\"Qual seu nome?\\nNão há sugestões.\"},\n",
        "      {\"role\":\"assistant\",\"content\":\"Meu nome é Recbot e estou que para te ajudar com as melhores dicas de restaurantes e pratos. Os mais gostosos da cidade\"}\n",
        "    ]\n",
        "    prompt_shots_user = prompt_shots + user_input_list\n",
        "    if self.flag_chat == 2:\n",
        "      for user_input in prompt_shots_user:\n",
        "        role = user_input['role']\n",
        "        content = user_input['content']\n",
        "        content = content.replace(\"'\", \"\")\n",
        "        content = content.replace('\"', '')\n",
        "        if role == 'user':\n",
        "          prompt += ', {\"role\":\"user\", \"content\": \"'\n",
        "        elif role == 'system':\n",
        "          prompt += '[{\"role\":\"system\", \"content\": \"'\n",
        "        else:\n",
        "          prompt += ', {\"role\":\"assistant\", \"content\": \"'\n",
        "        prompt += content + '\"}'\n",
        "      prompt += ']'\n",
        "      prompt = prompt.replace('\\n', ' ')\n",
        "\n",
        "    elif self.flag_chat == 1:\n",
        "      prompt = self.tokenizer.apply_chat_template(prompt_shots_user, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\")\n",
        "    else :\n",
        "      for user_input in prompt_shots_user:\n",
        "        role = user_input['role']\n",
        "        content = user_input['content']\n",
        "        if role == 'user':\n",
        "          prompt += \"user: {}\\n\".format(content)\n",
        "        elif role == 'system':\n",
        "          prompt += \"{}\\n\\n\".format(content)\n",
        "        else:\n",
        "          prompt += \"Recbot: {}\\n\\n\".format(content)\n",
        "      prompt += \"Recbot:\"\n",
        "    return prompt\n",
        "\n",
        "class gguf_model(standard_model_eval):\n",
        "  def __init__(self, model_id, retriever):\n",
        "    super().__init__(model_id, retriever)\n",
        "\n",
        "  def create(self):\n",
        "    l_index = self.model_id.find('/') + 1\n",
        "    n_index = self.model_id[l_index:].find('/')\n",
        "    f_index = l_index + n_index\n",
        "    model_language = self.model_id[:f_index]\n",
        "    model_file = self.model_id[f_index + 1:]\n",
        "    self.model = Llama.from_pretrained(repo_id = model_language, filename = model_file, verbose = False, n_ctx = 2048, n_gpu_layers = N_GPU_LAYERS)\n",
        "\n",
        "  def generate_text(self, prompt):\n",
        "    print(prompt)\n",
        "    outputs = self.model(prompt, max_tokens = N_TOKENS, stop = self.stop_strings, temperature=1.0, echo = False)\n",
        "    text = outputs['choices'][0]['text']\n",
        "    return text\n",
        "\n",
        "class bullerwins_model(standard_model_eval):\n",
        "  def __init__(self, model_id, retriever):\n",
        "    super().__init__(model_id, retriever)\n",
        "\n",
        "  def create(self):\n",
        "    hf_hub_download(repo_id=\"bullerwins/Meta-Llama-3.1-8B-Instruct-GGUF\", filename=\"Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\", revision=\"a4ac94cf28701b385c9028d49d314a361e0974a6\", local_dir=\"/content\")\n",
        "    self.model = Llama(model_path=\"/content/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\", verbose = False, n_batch = 256, n_ctx = 2048, n_gpu_layers = N_GPU_LAYERS)\n",
        "\n",
        "  def generate_text(self, prompt):\n",
        "    print(prompt)\n",
        "    outputs = self.model(prompt, max_tokens = N_TOKENS, stop = self.stop_strings, temperature=1.0, echo = False)\n",
        "    text = outputs['choices'][0]['text']\n",
        "    return text\n",
        "\n",
        "class second_state_model(standard_model_eval):\n",
        "  def __init__(self, model_id, retriever):\n",
        "    super().__init__(model_id, retriever, flag_chat = 2)\n",
        "\n",
        "  def create(self):\n",
        "    !{'curl -LO https://github.com/second-state/llamaedge/releases/latest/download/llama-api-server.wasm'}\n",
        "\n",
        "    def start_server():\n",
        "      l_index = self.model_id.find('/') + 1\n",
        "      n_index = self.model_id[l_index:].find('/')\n",
        "      f_index = l_index + n_index\n",
        "      model_language = self.model_id[:f_index]\n",
        "      model_file = self.model_id[f_index + 1:]\n",
        "      c1 = f'curl -LO https://huggingface.co/{model_language}/resolve/main/{model_file}'\n",
        "      c2 = f'wasmedge --dir .:. --nn-preload default:GGML:AUTO:{model_file} llama-api-server.wasm --prompt-template mistral-instruct --socket-addr 0.0.0.0:8000 --temp 1.0 --n-predict {N_TOKENS} --ctx-size 2048 --n-gpu-layers {N_GPU_LAYERS}'\n",
        "      !{c1}\n",
        "      !{c2}\n",
        "\n",
        "    service = Process(target=start_server)\n",
        "    service.start()\n",
        "    while True:\n",
        "      test_server =  !curl http://localhost:8000\n",
        "      if test_server == []:\n",
        "        break\n",
        "      time.sleep(5)\n",
        "      pass\n",
        "\n",
        "  def generate_text(self, prompt):\n",
        "    print(prompt)\n",
        "    r = 'curl -X POST http://localhost:8000/v1/chat/completions -H \\'accept:application/json\\' -H \\'Content-Type: application/json\\' -d \\'{\"messages\":' + prompt + '}\\''\n",
        "    response = !{r}\n",
        "    json_response = json.loads(response[0])\n",
        "    text = json_response['choices'][0][\"message\"][\"content\"]\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvAULQZgCTSo"
      },
      "outputs": [],
      "source": [
        "def get_retriever(retriver_model_name = \"jmbrito/ptbr-similarity-e5-small\"):\n",
        "  MONGO_URI = \"YOUR_URI\"\n",
        "  DATABASE_NAME = \"YOUR_DB\"\n",
        "  COLLECTION_NAME = \"YOUR_COLLECTION\"\n",
        "  ATLAS_VECTOR_SEARCH_INDEX_NAME = \"YOUR_VECTOR_INDEX\"\n",
        "  mongo_client = MongoClient(MONGO_URI)\n",
        "  mycol = mongo_client[DATABASE_NAME][COLLECTION_NAME]\n",
        "  embeddings = HuggingFaceEmbeddings(model_name=retriver_model_name)\n",
        "  vectore_store = MongoDBAtlasVectorSearch(collection= mycol, embedding=embeddings, index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME)\n",
        "  return vectore_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PZzRkvzorzq"
      },
      "outputs": [],
      "source": [
        "def get_model(model_name, retriever_model):\n",
        "  if 'second-state' in model_name:\n",
        "    return second_state_model(model_name, retriever_model)\n",
        "  elif 'bullerwins' in model_name:\n",
        "    return bullerwins_model(model_name, retriever_model)\n",
        "  elif '.gguf' in model_name:\n",
        "    return gguf_model(model_name, retriever_model)\n",
        "  url_hugging = 'https://huggingface.co/'\n",
        "  response = urllib3.request('GET', 'https://huggingface.co/{}'.format(model_name))\n",
        "  html_doc =  response.data.decode('utf-8')\n",
        "  soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "  list_a = soup.find_all(\"a\", {\"class\": \"mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg\"}, href=True)\n",
        "  list_names_language = ['bloom','falcon','xglm','mistral','stablelm','gpt2', 't5','llama','nvidia']\n",
        "\n",
        "  for a in list_a:\n",
        "    text_a = a.getText().strip('\\n')\n",
        "    if text_a in list_names_language:\n",
        "      return standard_model_eval(model_name, retriever_model)\n",
        "\n",
        "  return standard_model_eval(model_name, retriever_model, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75UPhcXUEOLv"
      },
      "outputs": [],
      "source": [
        "battles_chatbots = {\n",
        "    'battle 01': ['second-state/Mistral-Nemo-Instruct-2407-GGUF/Mistral-Nemo-Instruct-2407-Q5_0.gguf','bullerwins/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct-Q3_K_L.gguf'],\n",
        "    'battle 02': ['lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct-IQ4_XS.gguf','bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct-IQ4_XS.gguf'],\n",
        "    'battle 03': ['MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct.Q5_K_M.gguf','second-state/Mistral-Nemo-Instruct-2407-GGUF/Mistral-Nemo-Instruct-2407-Q5_0.gguf'],\n",
        "    'battle 04': ['bullerwins/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct-Q3_K_L.gguf','lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct-IQ4_XS.gguf'],\n",
        "    'battle 05': ['bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct-IQ4_XS.gguf','MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct.Q5_K_M.gguf'],\n",
        "    'battle 06': ['second-state/Mistral-Nemo-Instruct-2407-GGUF/Mistral-Nemo-Instruct-2407-Q5_0.gguf','lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct-IQ4_XS.gguf'],\n",
        "    'battle 07': ['bullerwins/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct-Q3_K_L.gguf','MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct.Q5_K_M.gguf'],\n",
        "    'battle 08': ['bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct-IQ4_XS.gguf','second-state/Mistral-Nemo-Instruct-2407-GGUF/Mistral-Nemo-Instruct-2407-Q5_0.gguf'],\n",
        "    'battle 09': ['lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct-IQ4_XS.gguf','MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct.Q5_K_M.gguf'],\n",
        "    'battle 10': ['bullerwins/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct-Q3_K_L.gguf','bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct-IQ4_XS.gguf']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmIyKXCaaRPg"
      },
      "outputs": [],
      "source": [
        "battle_atual = battles_chatbots[BATTLE_NAME]\n",
        "retriever = get_retriever()\n",
        "model = get_model(battle_atual[1], retriever)\n",
        "model.create()\n",
        "torch.cuda.empty_cache()\n",
        "anvil.server.connect(UP_LINK)\n",
        "name_file = 'chatbot_arena_chat2_{}.csv'.format(BATTLE_NAME)\n",
        "with open(name_file, 'w') as f:\n",
        "  writer = csv.writer(f)\n",
        "  writer.writerow(['USER','CHATBOT 2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCJQqnX07fks"
      },
      "outputs": [],
      "source": [
        "sessions = {}\n",
        "\n",
        "@anvil.server.callable\n",
        "def get_conversation_2():\n",
        "  #session_id = anvil.server.get_session_id()\n",
        "  session_id = 'chat_2'\n",
        "  if session_id not in sessions:\n",
        "    sessions[session_id] = {}\n",
        "    session = sessions[session_id]\n",
        "  else:\n",
        "    session = sessions[session_id]\n",
        "  chatBot_id = 'chatbot 2'\n",
        "  if chatBot_id not in session:\n",
        "    session[chatBot_id] = []\n",
        "  return session[chatBot_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnWqikKSK-p5"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def set_conversation_2(chat):\n",
        "  #session_id = anvil.server.get_session_id()\n",
        "  session_id = 'chat_2'\n",
        "  session = sessions[session_id]\n",
        "  chatBot_id = 'chatbot 2'\n",
        "  session[chatBot_id] = chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fq6aMHgL2TZ2"
      },
      "outputs": [],
      "source": [
        "@anvil.server.background_task\n",
        "def get_model_response_2(text):\n",
        "  chat_2 = get_conversation_2()\n",
        "  input_user = text + '\\n' + model.get_sugestions(text)\n",
        "  if len(chat_2) > 4:\n",
        "    chat_2 = chat_2[-4:]\n",
        "    get_conversation_2().append({\"role\": \"user\", \"content\": input_user})\n",
        "  chat_2.append({\"role\": \"user\", \"content\": input_user})\n",
        "  model_2_response = model.get_response(chat_2)\n",
        "  with open(name_file, 'a') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([text, model_2_response])\n",
        "  get_conversation_2().append({\"role\": \"assistant\", \"content\": model_2_response})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3v0S5TofOX7A"
      },
      "outputs": [],
      "source": [
        "num_chat_1 = 0\n",
        "num_chat_2 = 0\n",
        "\n",
        "@anvil.server.callable\n",
        "def set_result_2(chat_id):\n",
        "  with open(name_file, 'a') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['----------------', '----------------'])\n",
        "\n",
        "  global num_chat_1\n",
        "  global num_chat_2\n",
        "  if chat_id == 'Chatbot 1':\n",
        "    num_chat_1 += 1\n",
        "  else:\n",
        "    num_chat_2 += 1\n",
        "\n",
        "  os.system('cls')\n",
        "  print(f'{battle_atual[0]}: {num_chat_1}\\n{battle_atual[1]}: {num_chat_2}')\n",
        "  sessions['chat_2']['chatbot 2'] = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooGpTT_unEMF"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def close_server_2():\n",
        "  anvil.server.disconnect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CNDuEiNz29fN"
      },
      "outputs": [],
      "source": [
        "anvil.server.wait_forever()"
      ]
    }
  ]
}