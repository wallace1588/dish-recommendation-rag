{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNq4tPjA5ZJ/jwmQWyZ/RGB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wallace1588/tcc-dish-recommendation-rag/blob/main/notebooks/llm_as_a_judge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emfs9H3bH_-k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "86697bfedfe841bd9fdc725c46364350",
            "175f2645b9884333abdd1278dd7fe9e0",
            "7a06a8d54ed2468eb1d337e9f9851268",
            "51790d95c5964622ac204bcf896068c2",
            "87b9c0c79aa0460389e71dfd44c66461",
            "bc2ce23aa9c54e4ca4f85dfaa9eb2667",
            "ee39913db23649d1b2edb21baeafbc7e",
            "982529e54de04659900a835c9ece28cc",
            "2a7c440986e345afa16858471742abbc",
            "d9a402ada3174e5990ab0ec5a9301ad1",
            "6fa9aafb1d6849da9a475effc82928a6",
            "711e925db34e48f3a860383e1b754c61",
            "1adb8ebb881f414b9f30fcf0c1d084b5",
            "890443ace35144e58758b905b3280d01",
            "db06c51c91394a6c902d030ba9593ec5",
            "98df8512f1bf43b19129134ed83bae39",
            "743ba12f318149ee94127c14577781c7",
            "3ed4a1f1ba3543f6a1e3cc4fe788f60e",
            "ed765a2ad34e45169afd078196299d2d",
            "48e3c6e5b0154e1d9800dab0c4d64f57",
            "ea4565090d4a46bf808cfbe3f6542ea9",
            "cb5b1c8f2ac34370b97a94defc24dd34",
            "27116cb574f946d5820c2664129d3757",
            "93af166fe07b4a02aa3790386cbe2702",
            "19d1ae22243a4d79bfb02a57fcc76f20",
            "9d274202eeca45ae8b18a89b02341656",
            "6f22e8928e2f474ca6c82ff63adc3d3f",
            "01d6e5334ee3455cabc8a6c7e0965464",
            "9554039fd6e446d98ae1de1787c349bd",
            "6d66ac734ed0452bb688cfa2b657564d",
            "02a461aa401d4d64a0077cc3fa01488c",
            "f3a47d56157f4a7096b575ba5d7b88eb",
            "85ad3122140f43ec83c86ada5ddda12d",
            "ab040fc92d674fca80f343adb9c5d9be",
            "2a30781dffe840a4814805bfdfd94007",
            "20c3de215ee1406ba78e146f999cbc3f",
            "93b1d847f9674d36922e5ab66bd74921",
            "ec8e18d7c6a74f529eed8105283a19fc",
            "0b2a9b2fca804e69819ddcee27e034a2",
            "da0f95202bae4a6b8e351f2b9c44d817",
            "bf17e7f987f14c7c9360d78537cb46cf",
            "8693c02c87c047959ccf4cdbe7126bb2",
            "dd8a2dfd7820413cbbc0c0ec1fcf1074",
            "0b6e8fe42e674eb1bb3d1de840b47763",
            "76d226c3137441be8818b7944e9cb31b",
            "32700eb21798478dbe540daadbd83cd5",
            "88b7a0af5daa44a398e07940569004bc",
            "d7a41f0a76a5475a992a45812a8eaf0d",
            "6fd8b7cedbfe4192a36db023c6d7675e",
            "e4747adbdb724b8fa83f9a665b690dda",
            "c4d83deb8c084be79d4d025dc8cd199d",
            "c218e9adc6ae417f9d017c4d87e4d3d9",
            "dda166acfb034508bc36b6c407451b56",
            "50d5a96ec9ea4ab4bcb713fd053061f7",
            "d2a06c9724cd46dda3fa1547670b750c",
            "677bba0a86a4493c81a93d79329a2483",
            "7053d90d390547698bf0b8f9a797b936",
            "0438082b49514fabb22cc1d6254cee75",
            "fa9d186f53ae4c54aa9801c161ae2a02",
            "a5ed9219a2eb4a6f9abe2ef7e6c52016",
            "44ca91765f694b2296880b362d7134ef",
            "a9dab50be5334dc184b27dd8d6232bc7",
            "d2caa54fff2542d29610aded905827ea",
            "095bdd050fc2446f8e06bc7a0385f0cf",
            "f51cfef7e088475facf9740e65eaaeb4",
            "dc3cff7ff4954ff4a13101d6532dfdf1",
            "6ca6a17e985545b1968c58720712cccd",
            "7039d6bfebc44025b32f76062820ada8",
            "3b727bb6900845bda68d985947520a9c",
            "976a6d1728c6441fa931c6918eb36cc6",
            "d3850d17328343ae9d9248f4fccff044",
            "2c6a8ee4f0ca401783b9ad332e3d6246",
            "876b29f2b6ab45e5910b7824999b6094",
            "2d1607751e5c4be29c266d551970461e",
            "d0891341eb774ec294fb3b1e1a288b58",
            "5299cef2fee04fef8cda810f736b8e14",
            "e5dd805924d144c2a25bd960920649a6",
            "e1f1ac1012104454bee385828c484e3b",
            "eda11a50e83b496ba3cec1b1efd9c38b",
            "3382fc5efb554beda3e0ddf363430dda",
            "3c30947da7f4413ebbafe6f87cd301e7",
            "c5f071fcdeda4d4f9f9ee0fe5a850124",
            "cf112e7ca2334c96a5cb969e2dd1d3a4",
            "39b4ec97ad784070be1349b11b3ea3a2",
            "9925488abcf14f32871c512b4c6d649e",
            "bed9743fd7ac4c14b9c6b5393b0a9982",
            "38a00e36ee5f406284c92ecbd2eacd99",
            "55a0731ee2ee4354a496658d50e53b81",
            "9f7fb3bce0314df6bf0a6e3e12f9d1bb",
            "e12ce8feef974e7f948e93fd3cf18338",
            "58bad48c26534c49b3deb00283527f44",
            "54df047abfa7472b8caa10a00328e86f",
            "62e84d2dd4a04ff1b6935fb43bae4be1",
            "775dd998cb1c4d91ad2aaa43fb08ec84",
            "51eb81fbc1ba4795843c1a59cf9cbef5",
            "aa8341930cd944a598c5cbcecd29672d",
            "27774066f9904c20b82edae2d05c2301",
            "acd278590aa24ac58df1ed2270ec1d1f",
            "3339a782ed5741e5bdb325020b3f654d",
            "105219f4b9bd4f4b890d1cfb422f9bec",
            "994d801801984a32808e4bf4d1b919a7",
            "1a3cf322a1574e68828bbdb33fdcf16a",
            "b2fda4fb35dc4022a7815d1729a441fd",
            "a7bc967098964985abee0a58c2c1e101",
            "9120efc6255c433faf5cf0a984e4913e",
            "91f3348485ae44aab00ed9cf01c805fb",
            "d3e165eff5df47248e4726be456dfa46",
            "38f3b37bcec34fcb9f1845ed95e8d833",
            "f0547566e685453f9160bbe42679f911",
            "a0c47e7bf3124b3591c58b62b38da9d7",
            "b2101c1392fa422bb716acebf1ccc58b",
            "b9dd8828b4cc43a5bf08637f5abb1c66",
            "d4eda30050cd4a3c8603f4f48e8ccaa4",
            "8545dd9ccb984331a5977498e02a24d1",
            "4306d42a04af47f9a282e058e37b9541",
            "a154f8fa8eed497e91ffbd7243f05e7e",
            "478829bc59ce42c9b0589b9190634979",
            "87a3a2d022c840dd9f7709c78188a45f",
            "c5676f235f45466f990c1b8166a78a10",
            "d63c57038fd747599fec9793e75f9201",
            "d9cae7a0e6a04c20a09a92c2e690384b",
            "1e3fda6220f74b278f8380d3bfd850bd",
            "b77872d8febd4822b1d748f4c5949f4d",
            "d98bb79df5cc452bb6a576bb98f555bc",
            "0d8b2ed9cc214b0b81014bff3d9e1aa6",
            "006d8d3224f04be8b4a9c869f46536e1",
            "759f0206ddfa4914983fdbd3273414e6",
            "073ed5da4e004e7d9a5b7da41c773fcd",
            "a6f2bc4f1df4420ca0967099623f91b5",
            "505acdf057154146ac7015ff51350254",
            "3f36f2900b03404db5b0b9510ab05541",
            "608d8691460845de82d2c0f83c938ec5",
            "10579eb9cc27404fa27a61e6653c78b6",
            "a25abb416faa4860b731f8aa22e3b9a3",
            "80e53214b78f4341bbbfb29f3094b43d",
            "4a65dd97b3b443389928ecaa576e58b7",
            "97c71f0006be431493b4cf088990b8d6",
            "996a55b7deff4b90b5d17ab9db740641",
            "dbb8fcf6e5aa449f8870fbfe58bbe22b",
            "1fa98df6f5d54bc0b6d858a462026b4b",
            "4cc99b7d13ab4533bb326ce33f17f1a2",
            "144c10d46b164ca1b82698661a9cf29b",
            "c27a904258ee44d688c5c2dce751e047",
            "bfff1423f85e47c9ace048ad2138db6b",
            "69c9dc24dff2436faf4cb81dcb76b0dd",
            "7458f0f69d5b4419b030aa5ca06c9ff0",
            "cfcaf4e6a18c477490e3429c5a1fafc5",
            "1d16a296a69b4a9a8fdb0452dbebb3fa",
            "ed676c2963d5491d943da26a667248aa",
            "4c7936c94cfa452ab5beb0f1eac53c2c",
            "b4da57f08f3f4f3e84606d533080d2ab",
            "6a5cc50c67584fdeb6f2dbdd1e768db1",
            "1e4f2293b6be4f8eaf506300e283849b",
            "86c3be820ab54ad599b1570fae5aef52",
            "f5b48afcd21543a58734adbb15190683",
            "89828d7789994cf1a6568460d48de3cf",
            "474ff5bf49a041b5851f8621b042b089",
            "6d025430e7eb4bdcbd5a097ff4c789de",
            "19d8c96cdef34fcb850a4672b9e7a985",
            "e57d76714da14a0483da7272ba488f11",
            "508193bfa71546a9b60f28f15de2edcc",
            "144bdc93a1f74382ade442ef7ee2f55d",
            "64311c7d0b50402aa5e7754fd1c29065",
            "0b48b729b98a4ef894fb150bfdadc868",
            "91792f8cd64d4cc4b1c25376821fadf6",
            "9392ca85a9ba4a9ea165e34f2b40e5f3",
            "63ae4138fd294dbcb73782e7c77f0f94",
            "43c42e464734455b8d2bcd31148997fb",
            "719bf143420747eeb018cce4aab892ea",
            "f7aed64e9110463a86a9eb2b421e1ddb",
            "4e54fabcf98c4d8d9462c32d90de2b2b",
            "3eed2c62e09c46f2a0402697ab937491",
            "6fe8c14767174af988a48012acab97f2",
            "c754f07ba7f84b6ab1e5a0ba8e1949ef",
            "53927f0c61f34892878f5c48679be4aa",
            "7a69ac9ff5f743a3bd327514bd38c6ed",
            "21b39371970342f9bec9c7e01b8362f3",
            "a4380cb80dc642f081a1be9e452bef25",
            "83b6e3b641294c36b130ab04eb6f22b1",
            "828842d36da2440fb5f15755021d60d8",
            "e6b1865b4712470185c1a4c428979c4f",
            "56450c81f0fb496dac2755d6d986eb1a",
            "bd221e7f58aa43839deab4b5bc29997d",
            "3f79dd52ef6947308c8096b0f2647aef",
            "b0d918bb29154bac9a4c3adab679a316",
            "ddfe09c66b7f49cdbf5952846f8cd4fe",
            "8afa296f33d14248b2c100b2596f1ce2",
            "c93687afdd21465a9a77cd9f51f17085",
            "d7a84ecadef84e67accdcac81fb075a2",
            "95057669d54e4dc7b0ddad4dde2d4dcf",
            "19546ec66ed54bffa32eec96f7a2d486",
            "f81a776a72c346928a93f6be8a4580c9",
            "3372c1f4015f4957b2fd88ddd17bacb0",
            "12b32d7e86504a908f94c1c506d8bad9",
            "c02b5a4dea294b48a862cdf37562dbc6",
            "725982106c214322aff459aa49bad3e7",
            "7a7ee23ff2d342ee8f473514ba5d6fda",
            "e60bf97fd6524f73a0b7740815e995e8",
            "86ce136f208f4ee2ad7f7eb0c7deb76f",
            "8d8e04e9b88e4251a754e1de6bfaf41b",
            "d981960157ca408fb4cc5d8ce8779e90",
            "43f00cb7418e4816a0fc6aa6179b5bc9",
            "d7755838a4e1455d99d0c2f553d663b6",
            "94beb1bb4e114192b297924f5f8da10c",
            "dc0d869c2d26431b97dd7115b695d27a",
            "be2b899492104657a791be9d8ebf545f",
            "d100dcae9a914fa2940cd0aef5006b3f",
            "acc25cf8062e4f7b9bee60ad8c4d3cef",
            "f8da5af1f0e74da8920193e1226b042e",
            "dfbf5464311e41ff912399743c1ce2cb",
            "9e0c7359ff87478ca5aa3aa56a38181e",
            "5d16fbe292224800bf83f5b37cb67ef9",
            "99927c39f65248cc8545ab672b1308e5",
            "76d0a3f53d40412c9b7986060fc4bf47",
            "9602b3f5c57a4dfea8d113c2690bef1d",
            "586b2263cb8c4b36b8e9586b7577dc83",
            "b62cbdb56fb546898ae1da71e68ecbd5",
            "953f008250bc4e1bb9979e2d62ed8a6e",
            "16ed85d27f904f598cc85abc74f1643d",
            "cb48efce1303418c80e231a807754022",
            "e0667a5258604eaf9d03633b16bb49a5",
            "6fa9207488384da3bfde2132d82e711d",
            "8e2d1fbd56e44b40ae46f9b11d8edd69",
            "21ffaed396f44f2284607d488d9d0270",
            "a6c846dcb51149f88eeedcda0192c7f5",
            "53689e05cb5f494d8106b5711c03f188",
            "c3b5d65fa901406fb6acc2efec2e2a64",
            "5f914268b6854c5da6aad741da3c6c31",
            "275893126dbf46bf826f970c74be366c",
            "3a1e6671aae646fcae6a770aa4ccefdb",
            "902ab1b6e4024d5ba0dcfcd857cf5950",
            "ab6b3ca8cac849149fe0cad7e573a69a",
            "e73aadf45ac84b5fb2a03866112fe54a",
            "f5bf358069ae46b781f297b4817166e9",
            "46b73ff53d1d4607ad7a62be98d61cc2",
            "7f7dcfd5b4d741afb623652a13449f06",
            "728dc8dc508e404ebe4c4c2b8f2482e0",
            "3dd4bb46035f48a9bf63bb83edff0b38",
            "0ac30d6b1a4c4108bc7824632cdfdc64",
            "4fa6e39d986a4617a97656cea774c3cd",
            "f6e1399d523249078d405823451905a1",
            "e17a3f20c84f423489ceed7ca2ad9262",
            "55386b7d44d8415aab0762959ad9498c",
            "f61785bdb14a4066bcd5c904ec85ec69",
            "95b034ad14c242e1bd01473538929c83",
            "51298786b705416ca19152f28d83e8df",
            "5ea4368eb1e44b9e883085afaf575e6b",
            "935cd173a18947d2b695dbcac5c04e31",
            "9654a8b4b56a4cfa856cc02861fa66ea",
            "be66922144b34bc88165b3c873c44d23",
            "22ff2424d9794a4f82b3c07d00df5c9f",
            "ba27431ee4d34632a4701d25bdef808e",
            "2cccb1c9ebba4640b5ac680b6c089d61",
            "b7d9683f272945bf8d7c75a5d6ce0e31",
            "5242c93dc7284b47a4b1f72db2224670",
            "a3c03fc813ef486a98365caec5894eeb",
            "2f06292f4a774f9cb20af18c40630845",
            "54f1a4465f2a46c7a73f8ebb4b2f77f2",
            "5f3410909fab438694f16b8b4bcd257d",
            "e69b2eae49f846639555eae17a518697",
            "7abf4f19f19947449581495248760d45",
            "6769b50d64f54a80b2e686db176d24fa",
            "a3f11867436441f3a84c91bc28e712a6",
            "e6ae939a56d74806ba432bc97db9edf6",
            "e48b3bfa76ae4655aa5850d3f7a156ac",
            "f42cf8a9741546f892a0a20bdf4f190f",
            "c319c45219444eeeb6006f44ecd0fde6",
            "17820f30b56b4ec395bdb92a66ec80dd",
            "2465d1e69cb543898da222c28055e17e",
            "18edc3edf1ca46018b33bc8acfe03a68",
            "ffdc383474244abf9b260c02c210ee25",
            "70ab0feaf2ad432b86894c86475b3913",
            "a38041327cd847ebb04de282374771fb",
            "096ad578ba4c4db185bbdf4a1d12cc41",
            "3ea53aa09bd040b0b8d4df15123c2a39",
            "5ccd55d534524c1e8ea4e86fedc02005"
          ]
        },
        "outputId": "e509e00b-f47b-443f-9ca8-0faf083f0a7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bartowski/Mistral-Nemo-Instruct-2407-GGUF/Mistral-Nemo-Instruct-2407-Q5_K_S.gguf\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Mistral-Nemo-Instruct-2407-Q5_K_S.gguf:   0%|          | 0.00/8.52G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f42cf8a9741546f892a0a20bdf4f190f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "list_models_sheet = get_list_sheet(\"A:A\")[:-1]\n",
        "for model_name in list_models_name:\n",
        "  if model_name in list_models_sheet:\n",
        "    continue\n",
        "  row_sheet_num = len(list_models_sheet) + 1\n",
        "  write_cell_sheet(model_name, 'A' + str(row_sheet_num))\n",
        "  for i in range(23):\n",
        "    write_cell_sheet('', get_column_letter(i + 3) + str(row_sheet_num))\n",
        "  list_models_sheet = get_list_sheet(\"A:A\")\n",
        "\n",
        "  lang = get_language_model_name(model_name)\n",
        "  model = None\n",
        "  match lang:\n",
        "    case 'GGUF':\n",
        "      model = gguf_model(model_name, row_sheet_num, list_questions, gpt_judge, retriever_model)\n",
        "    case 'cohere':\n",
        "      model = standard_model_eval(model_name, row_sheet_num, list_questions, gpt_judge, retriever_model, 1)\n",
        "    case 'second-state':\n",
        "      model = second_state_model(model_name, row_sheet_num, list_questions, gpt_judge, retriever_model)\n",
        "    case _:\n",
        "      model = standard_model_eval(model_name, row_sheet_num, list_questions, gpt_judge, retriever_model)\n",
        "\n",
        "  try:\n",
        "    model.execute()\n",
        "\n",
        "  except:\n",
        "    clear_output()\n",
        "    print(model_name)\n",
        "    write_cell_sheet('error', 'D' + str(row_sheet_num))\n",
        "    write_cell_sheet(str(traceback.format_exc()), 'E' + str(row_sheet_num))\n",
        "    print('')\n",
        "    print(traceback.format_exc())\n",
        "    print()\n",
        "\n",
        "  #Clean resources\n",
        "  model.clean()\n",
        "  del model\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  list_result_sheet = get_list_sheet(\"D:D\")\n",
        "  num_error = 0\n",
        "  num_succe = 0\n",
        "  for result in list_result_sheet:\n",
        "    if result == 'success':\n",
        "      num_succe += 1\n",
        "    if result == 'error':\n",
        "      num_error += 1\n",
        "\n",
        "  print('Sucessos: {}'.format(num_succe))\n",
        "  print('Erros: {}'.format(num_error))\n",
        "  time.sleep(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cabqpaInZUba",
        "outputId": "db7c744e-7baf-4086-c418-0809bbe5327b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
            "Collecting openai\n",
            "  Downloading openai-1.37.1-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting pymongo\n",
            "  Downloading pymongo-4.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.10-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.2-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting flash_attn\n",
            "  Downloading flash_attn-2.6.3.tar.gz (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.23.5)\n",
            "Collecting llama-cpp-python==0.2.82\n",
            "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.2.82-cu122/llama_cpp_python-0.2.82-cp310-cp310-linux_x86_64.whl (284.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.1/284.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.82) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.82) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.2.82)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.82) (3.1.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.42.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain<0.3.0,>=0.2.9 (from langchain-community)\n",
            "  Downloading langchain-0.2.11-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.3.0,>=0.2.23 (from langchain-community)\n",
            "  Downloading langchain_core-0.2.24-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.0 (from langchain-community)\n",
            "  Downloading langsmith-0.1.93-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting einops (from flash_attn)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2024.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.82) (2.1.5)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.9->langchain-community)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.23->langchain-community)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain-community)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.23->langchain-community)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Downloading openai-1.37.1-py3-none-any.whl (337 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymongo-4.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.2.10-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.43.2-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.2.11-py3-none-any.whl (990 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.24-py3-none-any.whl (377 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.3/377.3 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.1.93-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m53.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: flash_attn\n",
            "  Building wheel for flash_attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash_attn: filename=flash_attn-2.6.3-cp310-cp310-linux_x86_64.whl size=187290390 sha256=c50f5de67a8b75bcfcf4a34257622475f9b56d59da58a539476a21db9d5b9867\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/e3/c3/89c7a2f3c4adc07cd1c675f8bb7b9ad4d18f64a72bccdfe826\n",
            "Successfully built flash_attn\n",
            "Installing collected packages: orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, marshmallow, jsonpointer, h11, einops, dnspython, diskcache, typing-inspect, pymongo, nvidia-cusparse-cu12, nvidia-cudnn-cu12, llama-cpp-python, jsonpatch, httpcore, nvidia-cusolver-cu12, langsmith, httpx, dataclasses-json, openai, langchain-core, sentence-transformers, langchain-text-splitters, flash_attn, bitsandbytes, langchain, langchain-community\n",
            "Successfully installed bitsandbytes-0.43.2 dataclasses-json-0.6.7 diskcache-5.6.3 dnspython-2.6.1 einops-0.8.0 flash_attn-2.6.3 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.11 langchain-community-0.2.10 langchain-core-0.2.24 langchain-text-splitters-0.2.2 langsmith-0.1.93 llama-cpp-python-0.2.82 marshmallow-3.21.3 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 openai-1.37.1 orjson-3.10.6 pymongo-4.8.0 sentence-transformers-3.0.1 typing-inspect-0.9.0\n",
            "\u001b[0;33mUsing Python: /usr/bin/python3 \u001b[0m\n",
            "WARNING - Experimental Option Selected: plugins\n",
            "WARNING - plugins option may change later\n",
            "INFO    - Compatible with current configuration\n",
            "INFO    - Running Uninstaller\n",
            "WARNING - SHELL variable not found. Using bash as SHELL\n",
            "INFO    - shell configuration updated\n",
            "INFO    - Downloading WasmEdge\n",
            "|============================================================|100.00 %INFO    - Downloaded\n",
            "INFO    - Installing WasmEdge\n",
            "INFO    - WasmEdge Successfully installed\n",
            "INFO    - Downloading Plugin: wasi_logging\n",
            "|============================================================|100.00 %INFO    - Downloaded\n",
            "INFO    - Downloading Plugin: wasi_nn-ggml-cuda\n",
            "|============================================================|100.00 %INFO    - Downloaded\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "INFO    - Run:\n",
            "source /root/.bashrc\n"
          ]
        }
      ],
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install openai pymongo sentence-transformers langchain-community bitsandbytes flash_attn accelerate huggingface-hub llama-cpp-python==0.2.82 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
        "!curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install.sh | bash -s -- -p /usr/local --plugins wasi_logging wasi_nn-ggml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGFyGRUalNOY"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import MongoDBAtlasVectorSearch\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from pymongo.mongo_client import MongoClient\n",
        "from torch import bfloat16\n",
        "import torch\n",
        "import transformers\n",
        "import urllib3\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import userdata\n",
        "from IPython.display import clear_output\n",
        "import shutil\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from openpyxl.utils import get_column_letter\n",
        "from google.oauth2.credentials import Credentials\n",
        "from googleapiclient.discovery import build\n",
        "from llama_cpp import Llama\n",
        "from openai import AzureOpenAI\n",
        "import psutil\n",
        "import numpy as np\n",
        "import gc\n",
        "from multiprocessing import Process\n",
        "import psutil\n",
        "import traceback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZjWD0jklNOY"
      },
      "outputs": [],
      "source": [
        "def get_list_models_name():\n",
        "  list_url_models = []\n",
        "  num_pag = 0\n",
        "  url_hug_face = 'https://huggingface.co/'\n",
        "\n",
        "  while True:\n",
        "    response = urllib3.request('GET', '{}models?pipeline_tag=text-generation&language=pt&p={}&sort=downloads'.format(url_hug_face,str(num_pag)))\n",
        "    html_doc =  response.data.decode('utf-8')\n",
        "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "    list_a = soup.find_all(\"a\", {\"class\": \"flex items-center justify-between gap-4 p-2\"}, href=True)\n",
        "    if len(list_a) > 0:\n",
        "      for l in list_a:\n",
        "        model = l['href']\n",
        "        list_url_models.append(model[1:])\n",
        "    else:\n",
        "      break\n",
        "    num_pag += 1\n",
        "\n",
        "  list_models_name = []\n",
        "  for i in range(len(list_url_models)):\n",
        "    if i == 50:\n",
        "      break\n",
        "    url_model = list_url_models[i]\n",
        "    if url_model in ['CohereForAI/c4ai-command-r-plus', 'CohereForAI/aya-23-35B','CohereForAI/c4ai-command-r-v01','CohereForAI/c4ai-command-r-plus-4bit']:\n",
        "      continue\n",
        "    print(url_model)\n",
        "    response = urllib3.request('GET', '{}{}/tree/main'.format(url_hug_face, url_model))\n",
        "    html_doc =  response.data.decode('utf-8')\n",
        "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "\n",
        "    list_li = soup.find_all(\"li\", {\"class\": \"grid h-10 grid-cols-12 place-content-center gap-x-3 border-t px-3 dark:border-gray-800\"})\n",
        "    list_files_gguf = []\n",
        "    len_safetensors = 0\n",
        "    for li in list_li:\n",
        "      name_model = li.find(\"span\", {\"class\": \"truncate group-hover:underline\"})\n",
        "      if name_model == None:\n",
        "        continue\n",
        "      name_model = name_model.getText()\n",
        "      if \".gguf\" in name_model:\n",
        "        name_size_model = li.find(\"a\", {\"class\": \"group col-span-4 flex items-center justify-self-end truncate text-right font-mono text-[0.8rem] leading-6 text-gray-400 md:col-span-3 lg:col-span-2 xl:pr-10\"}).getText()\n",
        "        size_model = float(name_size_model[:name_size_model.find(' ')])\n",
        "        if (\"GB\" not in name_size_model) or (size_model < 11.2):\n",
        "          list_files_gguf.append(url_model + '/' + name_model)\n",
        "      elif \".safetensors\" in name_model:\n",
        "        name_size_model = li.find(\"a\", {\"class\": \"group col-span-4 flex items-center justify-self-end truncate text-right font-mono text-[0.8rem] leading-6 text-gray-400 md:col-span-3 lg:col-span-2 xl:pr-10\"}).getText()\n",
        "        if \"GB\" in name_size_model:\n",
        "          size_model = float(name_size_model[:name_size_model.find(' ')])\n",
        "          len_safetensors += size_model\n",
        "\n",
        "    if list_files_gguf != []:\n",
        "      list_models_name = list_models_name + list_files_gguf\n",
        "    elif len_safetensors < 40 and len_safetensors > 0:\n",
        "      list_models_name.append(url_model)\n",
        "    #clear_output()\n",
        "    #print(len(list_models_name))\n",
        "\n",
        "  #clear_output()\n",
        "  #print(\"Total de modelos: {}\".format(len(list_models_name)))\n",
        "  return list_models_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hirDAGuuDiIp"
      },
      "outputs": [],
      "source": [
        "def get_language_model_name(model_name):\n",
        "  if 'second-state' in model_name:\n",
        "    return 'second-state'\n",
        "  elif '.gguf' in model_name:\n",
        "    return 'GGUF'\n",
        "  url_hugging = 'https://huggingface.co/'\n",
        "  response = urllib3.request('GET', 'https://huggingface.co/{}'.format(model_name))\n",
        "  html_doc =  response.data.decode('utf-8')\n",
        "  soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "  list_a = soup.find_all(\"a\", {\"class\": \"mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg\"}, href=True)\n",
        "  list_names_language = ['GGUF','bloom','falcon','xglm','cohere','mistral','stablelm','gpt2', 't5','llama','nvidia']\n",
        "\n",
        "  for a in list_a:\n",
        "    text_a = a.getText().strip('\\n')\n",
        "    if text_a in list_names_language:\n",
        "      return text_a\n",
        "\n",
        "  return 'cohere'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_jHFIuhlNOY"
      },
      "outputs": [],
      "source": [
        "def get_list_questions():\n",
        "  jsonl = urllib3.request('GET', 'https://raw.githubusercontent.com/wallace843/recomender_dish_chatbot/main/data/recomender_dish_bench.jsonl')\n",
        "  list_text = jsonl.data.decode('utf-8').split('\\n')[:-1]\n",
        "\n",
        "  return [json.loads(l) for l in list_text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEaOE_ERlNOY"
      },
      "outputs": [],
      "source": [
        "def get_retriever(retriver_model_name = \"jmbrito/ptbr-similarity-e5-small\"):\n",
        "  MONGO_URI = \"mongodb+srv://consulta:UPbGavoAFWwu0aB8@cluster0.nexbe93.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "  DATABASE_NAME = \"mydatabase\"\n",
        "  COLLECTION_NAME = \"FoodRec_Dishes\"\n",
        "  ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\"\n",
        "\n",
        "  mongo_client = MongoClient(MONGO_URI)\n",
        "  mydb = mongo_client[DATABASE_NAME]\n",
        "  mycol = mydb[COLLECTION_NAME]\n",
        "\n",
        "  embeddings = HuggingFaceEmbeddings(model_name=retriver_model_name)\n",
        "\n",
        "  vectore_store = MongoDBAtlasVectorSearch(\n",
        "    collection= mycol,\n",
        "    embedding=embeddings,\n",
        "    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME\n",
        "  )\n",
        "  return vectore_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458,
          "referenced_widgets": [
            "128ed6db91f24335b4d10ccaad4ed126",
            "c3b9535a54564f0ba37429cc6cf032e8",
            "f47a1b7f595945c685944515c7970ab3",
            "d691c92d65714f85ad8e66004509a184",
            "cd48118401dc41cf923857ce3f71ceae",
            "3dd245ece6e74bbe95b8b84e530056c7",
            "026f4745e58241089b5dfe9e300c36f0",
            "a388d061b82b4c2093aa0c5fb511922a",
            "838af342c1b04a5db9a555b3615adeb6",
            "01937b7e6bf14d2cb5759a93a5134be2",
            "dd6c4118aee14a29a8d73465a7a673f3",
            "fcaad915c77349adbb861b69e40a6dc9",
            "5fc9aaf6e8bd4a978bc527c97c17f286",
            "79a2245d731e46ecb8b583db946757e8",
            "3b871b78cf924c9ea711267b4480b80b",
            "5b1dfa4cbea34834ab475ba3488e87c6",
            "9167f10d63b443ef8f728898e0366656",
            "d1ce4ef7ddff4522b725895b1b838fe3",
            "af9bd9bdf65e4d38b7d437e5b27f1cbb",
            "6dd5c323947446a090685b40f8063955",
            "24d3a820eee3412a88b31e38ff09ec0b",
            "0724b7b6d0574801b32b9fa7903d280b",
            "c5d3f51020534cf2966b3958341049af",
            "a7c1865656fc4372a5218dbee91bfcff",
            "6e7e297cce3e4f43b6256adde91025aa",
            "76f5c1e644df435abdf618d33d3dacb9",
            "aa194c77a89944319c2ff0fe9fbde28e",
            "4f65bd457c554aa5ad9d2019af67b164",
            "4b01616dcf7f498bbc40402c56192a4e",
            "80731bdd2a1e43408ddeaf7b661ba29b",
            "253817b6341a4e9cb547f33cba698c66",
            "769821feddff456dae5391b77c72f45e",
            "8039aaf1525f4497a477f53381c227b0",
            "b027533ca8b145cf824c8f79ed6286d2",
            "b6c87ddfdb5b47a89c818ec4958146e0",
            "27de23039e9843918f770245ae97f2b6",
            "1c7d0a6bf9d34e22abde5a02c5194f56",
            "1bd283780e1f440a81f468cae5891108",
            "c20f627331d647f0880e89fa01f5129d",
            "51c4ba933caf4409b44ebc7457f7f2ea",
            "5236e923476c494d876f7e21dac1f69b",
            "7838c3cfe6744a28895b21c3d92fc59d",
            "c6b72b46e47746ab8a03e2d3be868311",
            "bc111282822a4ee0883ddef47051cb42",
            "afb15600955b4ca7b0d9da55fcfe0640",
            "c36688e18b254e34aabc8de84593b48c",
            "08758191b0f64cd7b1794c62ccc14595",
            "b05be4766eaa4497a13a31d18d5e8d97",
            "5937a8d11f774e518ea96efab200791c",
            "c8405a112766470ba81464fde83c1171",
            "1ded34da8a2d4617aaea29177937c931",
            "333153cf42ab4fc486e06e97e2e6df2b",
            "a86a86d73cfe44d99e0f55122cde92db",
            "ca18b2b65bbb43729a848ce898683b37",
            "02a85a71bf164b21895c6e6db71be464",
            "ada6aa6cfe7b49ad81004a39dd0834cd",
            "52dab723750a47f9813c8736b8e1386e",
            "a8a2f2a2e8b3443c8eb84e1af9171fcb",
            "7c6bbe1c672c446cab8c6c01159484c1",
            "c2a6c27963a543a899b686965e937490",
            "14cc636d1f1147d0a2b14b0bda7ef759",
            "500400fbea1f492fa161a2bb6bbf5c70",
            "8b180c7e02634abf94229505ff075fb5",
            "dfa101124f36440297d6138c4541df42",
            "bc50a499171a44b589b081d6d2878285",
            "4404739ed2934d4abef31ed7267a56b0",
            "896490cae14e4e9fa98866f0b6635a01",
            "28d6036662e643938c47e4fdf7c3ae54",
            "c9a6aca7952d458a84da613788d7b023",
            "77b024cf778142a486707ade34a66a94",
            "e2700c7cea0c4665b6ba232bb586e1f2",
            "cd4d6f2614c44974ba4a9d5652a3fab6",
            "f77d1569b66e497c81e9ca07a61d1fe0",
            "3c70247d6ecf4db6b3bc8cd8d822a820",
            "b2aca921b9c94e258036f824f3670501",
            "ccfbfacce0d14c2d90b04380d3637e51",
            "9048162818484d08a585a6758b098942",
            "9d6b886a440546f1bc68d3b3e8a8b10e",
            "a98eb20f740c436d9ebee27332a407da",
            "de460222cc68421693e453d46fbbeb74",
            "f907c19989404173b6c93dd858f45337",
            "a3677b61dab54092a1c099ea23bcef9c",
            "b8e65de304284679a4ff047956bb573c",
            "f71f353de19644639ec31b8cf1626c07",
            "57b9030a85b64d15a360d79988e2dfec",
            "63ed896f72364fdd9e711366e66a0193",
            "aa288828ac6e488c83c3311c7971eb39",
            "93665b60e72142afa1592b55fb4722db",
            "f7068393176f4f1fbd50b2affc6178d9",
            "a334b30a98df42c3901b573a2dfee3fd",
            "7f11b3236d9f44db8aeaed2dd9ccbb09",
            "8c831061617c4064b6012221d75e4ad5",
            "e283922f53b24538b7656063cf4ab1a0",
            "4ea2b4c0d03f45369f1ad7e2825c425c",
            "2a4ec61bb3f240da8aa08e99c8f201fb",
            "880343e541f44575af1aa9ed26338f9a",
            "3d35d0349db544e092c9ca23434b2075",
            "5de0351a35ee446ab259959a55328f5d",
            "9411350966374019bcd4694c517a1ed4",
            "a1c0602a5ef842099f91b070d74a2019",
            "d120d3538aa64151897967a8474db484",
            "0f2b0755b1284a0aa8aae565266da742",
            "4022078f725e4475b4e65adc538cc5c7",
            "c89f33434de24f42997dbbd276dbde1e",
            "9269bec37931453898a2deeeaf8fd296",
            "2ea72fec22a74b5fbb82eeddd0717ff9",
            "674b85be2fd6474eac0fd9211c416389",
            "a412fb08905541a0885ae0f23d2f0417",
            "22cc1f8098d646eb9e68d9b268c63dd4",
            "267d80b0784541e48ea50a49cf836255",
            "dc504a68f8e14010bb67d568c86a956b",
            "d67955f31d7e4015a73e372e3534bf9c",
            "3467d32a2ec34ca5b147f48a6963a473",
            "fd9f6f0e2e9d4594a161f68dbb1ebce5",
            "cdfd3377f6874afeae2ba74e18262e9f",
            "6e7a0a41a61a413999d84a08e76b0041",
            "465f8a06be774891a66ccc381b2235df",
            "05206546198342958e7defaef02e287c",
            "69f326c3859e4d55b8dbc9c0b2952da2",
            "10aee6da7d474fd29301b87172713590",
            "3843faaa43ea47aba16ae032aa30046c"
          ]
        },
        "id": "20C85Q2Hs1Fl",
        "outputId": "e0227a8b-2e2c-4045-c15b-bbf2f8eecbd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "128ed6db91f24335b4d10ccaad4ed126"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fcaad915c77349adbb861b69e40a6dc9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/2.59k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5d3f51020534cf2966b3958341049af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b027533ca8b145cf824c8f79ed6286d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/672 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afb15600955b4ca7b0d9da55fcfe0640"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/471M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ada6aa6cfe7b49ad81004a39dd0834cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/634 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "896490cae14e4e9fa98866f0b6635a01"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d6b886a440546f1bc68d3b3e8a8b10e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7068393176f4f1fbd50b2affc6178d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/167 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1c0602a5ef842099f91b070d74a2019"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc504a68f8e14010bb67d568c86a956b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `MongoDBAtlasVectorSearch` was deprecated in LangChain 0.0.25 and will be removed in 0.3.0. An updated version of the class exists in the langchain-mongodb package and should be used instead. To use it run `pip install -U langchain-mongodb` and import as `from langchain_mongodb import MongoDBAtlasVectorSearch`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "vectore_store = get_retriever()\n",
        "def get_sugestions(input_text):\n",
        "\n",
        "  list_docs = vectore_store.similarity_search_with_score(input_text, 3)\n",
        "\n",
        "  THRESHOLD_MIN = 0.825\n",
        "  intro_sugestions = 'Sugestões: '\n",
        "  sugestions = intro_sugestions\n",
        "  for r in list_docs:\n",
        "    dish_description = r[0].page_content\n",
        "    if r[1] > THRESHOLD_MIN:\n",
        "      sugestions = sugestions + dish_description + '; '\n",
        "  if sugestions == intro_sugestions:\n",
        "    sugestions = 'Não há sugestões.'\n",
        "  return sugestions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-U3PH1b_dNf"
      },
      "outputs": [],
      "source": [
        "def get_colab_stats():\n",
        "  gpu_used = !nvidia-smi --query-gpu=memory.used --format=csv\n",
        "  gpu_used = gpu_used[1]\n",
        "  gpu_used = float(gpu_used[:gpu_used.find(' MiB')])\n",
        "  mem_used = psutil.virtual_memory().used/pow(1024,3)\n",
        "  disk_used = psutil.disk_usage('/').used/pow(1024,3)\n",
        "\n",
        "  return [disk_used, mem_used, gpu_used]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZSYCmEJPTQv"
      },
      "outputs": [],
      "source": [
        "with open(\"token.json\", \"w\") as outfile:\n",
        "  outfile.write('YOUR-JSON-TOKEN')\n",
        "\n",
        "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\"]\n",
        "creds = Credentials.from_authorized_user_file(\"token.json\", SCOPES)\n",
        "service = build(\"sheets\", \"v4\", credentials=creds)\n",
        "sheet = service.spreadsheets()\n",
        "SHEET_ID = '1wnFJU2_o89rq3ojk04RW3-aTe68HGpeEpi9wlCmmbSM'\n",
        "\n",
        "def write_cell_sheet(text, cell_range):\n",
        "  while True:\n",
        "    try:\n",
        "      #print(\"Text: {} range: {}\".format(text, cell_range))\n",
        "      #range = get_column_letter(column_int) + str(line_int)\n",
        "      sheet.values().update(spreadsheetId=SHEET_ID, range=cell_range, valueInputOption=\"USER_ENTERED\", body={\"values\":[[text]]}).execute()\n",
        "      break\n",
        "    except:\n",
        "      time.sleep(2)\n",
        "      continue\n",
        "\n",
        "def get_list_sheet(range):\n",
        "  result = sheet.values().get(spreadsheetId=SHEET_ID, range=range).execute()\n",
        "  values = result.get(\"values\", [])\n",
        "  list_model = []\n",
        "  for value in values:\n",
        "    if value == []:\n",
        "      break\n",
        "\n",
        "    model_name = value[0]\n",
        "\n",
        "    if model_name != '':\n",
        "      list_model.append(model_name)\n",
        "\n",
        "  return list_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzckmnGa9oQF"
      },
      "outputs": [],
      "source": [
        "class standard_model_eval:\n",
        "  def __init__(self, model_id, row_sheet_num, list_questions, llm_judge, retriever, flag_chat = 0, stop_strings = [\"\\n\", \"user:\"]):\n",
        "    self.model_id = model_id\n",
        "    self.row_sheet_num = row_sheet_num\n",
        "    self.flag_chat = flag_chat\n",
        "    self.stop_strings = stop_strings\n",
        "    self.list_questions = list_questions\n",
        "    self.llm_judge = llm_judge\n",
        "    self.retriever = retriever\n",
        "    self.tokenizer = None\n",
        "    self.model = None\n",
        "\n",
        "  def create(self):\n",
        "    bnb_config = transformers.BitsAndBytesConfig(load_in_4bit = True, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=bfloat16)\n",
        "    self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_id, token=HF_TOKEN)\n",
        "    self.model = transformers.AutoModelForCausalLM.from_pretrained(self.model_id, quantization_config=bnb_config, token=HF_TOKEN)\n",
        "\n",
        "  def generate_text(self, prompt):\n",
        "    inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    pad_token_id = self.tokenizer.eos_token_id if self.tokenizer.pad_token_id is None else self.tokenizer.pad_token_id\n",
        "    outputs = self.model.generate(inputs, max_new_tokens = 300, stop_strings = self.stop_strings, tokenizer = self.tokenizer, pad_token_id=pad_token_id)\n",
        "    outputs =  outputs[0:,len(inputs[0]):]\n",
        "    output_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return output_text\n",
        "\n",
        "  def get_response(self, input_text):\n",
        "    text = self.generate_text(self.prompt_gen(input_text))\n",
        "    response = ''\n",
        "    for stop_string in self.stop_strings:\n",
        "      if text.find(stop_string) < 0:\n",
        "        continue\n",
        "      else:\n",
        "        response = text[:text.find(stop_string)].strip('\\n')\n",
        "        break\n",
        "    if response == '':\n",
        "      response = text.strip('\\n')\n",
        "    return response\n",
        "\n",
        "  def get_sugestions(self, input_text):\n",
        "    THRESHOLD_MIN = 0.825\n",
        "    list_docs = self.retriever.similarity_search_with_score(input_text, 3)\n",
        "    intro_sugestions = 'Sugestões: '\n",
        "    sugestions = intro_sugestions\n",
        "    for r in list_docs:\n",
        "      dish_description = r[0].page_content\n",
        "      if r[1] > THRESHOLD_MIN:\n",
        "        sugestions = sugestions + dish_description + '; '\n",
        "    if sugestions == intro_sugestions:\n",
        "      sugestions = 'Não há sugestões.'\n",
        "    return sugestions\n",
        "\n",
        "  def get_colab_stats(self):\n",
        "    gpu_used = !nvidia-smi --query-gpu=memory.used --format=csv\n",
        "    gpu_used = gpu_used[1]\n",
        "    gpu_used = float(gpu_used[:gpu_used.find(' MiB')])\n",
        "    mem_used = psutil.virtual_memory().used/pow(1024,3)\n",
        "    disk_used = psutil.disk_usage('/').used/pow(1024,3)\n",
        "    return [disk_used, mem_used, gpu_used]\n",
        "\n",
        "  def prompt_gen(self, user_input_list):\n",
        "    prompt = \"\"\n",
        "    prompt_shots = [\n",
        "      {\"role\":\"system\",\"content\":\"Seu nome é Recbot e você deve fazer recomendações para auxiliar os usuários na escolha de refeições e restaurantes. Use as informações de entrada do usuário para gerar sua resposta. Caso não haja sugestões, você deve solicitar mais informações ao usuário ou responder baseado no histórico da conversa.\"},\n",
        "      {\"role\":\"user\",\"content\":\"Estou com fome.\\nNão há sugestões.\"},\n",
        "      {\"role\":\"assistant\",\"content\":\"Estou aqui para te ajudar. O que você gosta de comer?\"},\n",
        "      {\"role\":\"user\",\"content\":\"queria tomar guarana jesus.\\nSugestões: Guarana Jesus: lata 350ml, 7.15, Pasta in Cup, 4.9, https://www.ifood.com.br/delivery/recife-pe/pasta-in-cup-torre/2d4543fd-cb1b-4cb8-b4c8-7c778dfacc0c, ifood; Refrigerante Guarana Jesus 350ml: lata 350ml, 7.0, Donico Coxinhas Artesanais Com Massa de, 4.9, https://www.ifood.com.br/delivery/recife-pe/donico-coxinhas-artesanais-com-massa-de-prado/ce9fffb6-23f4-4117-a604-9ff71ba78405, ifood; Refrigerante Jesus Guarana Lata 350ml: lata 350ml, 4.6, Padaria Ipiranga, 4.9, https://www.ifood.com.br/delivery/recife-pe/padaria-ipiranga-afogados/eb4b66e2-8469-46d5-acf0-b905326fe38f, ifood;\"},\n",
        "      {\"role\":\"assistant\",\"content\":\"Uma boa opção é o Guaraná Jesus 350ml da Padaria Ipiranga. Ele está disponível no Ifood e custa apenas R$ 4.60. Para mais informações acesse o link: https://www.ifood.com.br/delivery/recife-pe/padaria-ipiranga-afogados/eb4b66e2-8469-46d5-acf0-b905326fe38f.\"},\n",
        "      {\"role\":\"user\",\"content\":\"Onde posso comprar?\\nNão há sugestões.\"},\n",
        "      {\"role\":\"assistant\",\"content\":\"Você pode comprar direto na Padaria Ipiranga ou através do site: https://www.ifood.com.br/delivery/recife-pe/padaria-ipiranga-afogados/eb4b66e2-8469-46d5-acf0-b905326fe38f.\"}\n",
        "    ]\n",
        "    prompt_shots_user = prompt_shots + user_input_list\n",
        "    if self.flag_chat == 2:\n",
        "      for user_input in prompt_shots_user:\n",
        "        role = user_input['role']\n",
        "        content = user_input['content']\n",
        "        content = content.replace(\"'\", \"\")\n",
        "        content = content.replace('\"', '')\n",
        "        if role == 'user':\n",
        "          prompt += ', {\"role\":\"user\", \"content\": \"'\n",
        "        elif role == 'system':\n",
        "          prompt += '[{\"role\":\"system\", \"content\": \"'\n",
        "        else:\n",
        "          prompt += ', {\"role\":\"assistant\", \"content\": \"'\n",
        "        prompt += content + '\"}'\n",
        "      prompt += ']'\n",
        "      prompt = prompt.replace('\\n', ' ')\n",
        "\n",
        "    elif self.flag_chat == 1:\n",
        "      prompt = self.tokenizer.apply_chat_template(prompt_shots_user, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\")\n",
        "    else :\n",
        "      for user_input in prompt_shots_user:\n",
        "        role = user_input['role']\n",
        "        content = user_input['content']\n",
        "        if role == 'user':\n",
        "          prompt += \"user: {}\\n\".format(content)\n",
        "        elif role == 'system':\n",
        "          prompt += \"{}\\n\\n\".format(content)\n",
        "        else:\n",
        "          prompt += \"Recbot: {}\\n\\n\".format(content)\n",
        "      prompt += \"Recbot:\"\n",
        "    return prompt\n",
        "\n",
        "  def rating_conversation(self, conversation):\n",
        "    prompt_judge = [{\"role\":\"system\",\"content\":\"Por favor aja como um juiz imparcial e avalie a qualidade do conjunto das respostas geradas por um assistente IA para as questões do usuário mostradas abaixo. Caso hajam sugestões, o assistente de IA poderá elaborar recomendações usando as sugestões dadas como contexto. Caso não hajam sugestões, o assistente de IA poderá interagir com o usuário para coletar mais informações ou elaborar suas respostas baseado no histórico da conversa. Sua avaliação deve considerar fatores como utilidade, relevância, precisão, criatividade, profundidade e nível de detalhe do conjunto das respostas. Comece sua avaliação provendo uma pequena explanação. Seja o mais objetivo possível. Após prover sua explanação, por favor avalie do conjunto das respostas numa escala de 0 a 100 seguindo estritamente este formato: '[[[rating]]]', por exemplo: 'Avaliação: [[[5]]]'\"}]\n",
        "    user_input = \"\"\n",
        "    for c in conversation:\n",
        "      if c['role'] == 'user':\n",
        "        user_input += \"User: {}\\n\".format(c['content'])\n",
        "      elif c['role'] == 'assistant':\n",
        "        user_input += \"Assistant: {}\\n\".format(c['content'])\n",
        "    user_input = \"<|Inicio da conversa do Assistente com o Usuário|>\\n\" + user_input + \"<|Fim da conversa do Assistente com o Usuário|>\"\n",
        "    prompt_judge.append({\"role\":\"user\",\"content\":user_input})\n",
        "    completion = self.llm_judge.chat.completions.create(\n",
        "      model=\"gpt-4\",\n",
        "      messages = prompt_judge,\n",
        "      temperature=0,\n",
        "      max_tokens=1000,\n",
        "      top_p=0.95,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0,\n",
        "      stop=None\n",
        "    )\n",
        "    full_rating = completion.choices[0].message.content\n",
        "    try:\n",
        "      rating_value = int(full_rating[full_rating.find('[[[')+3:full_rating.find(']]]')])\n",
        "      return rating_value\n",
        "    except:\n",
        "      return 15\n",
        "\n",
        "  def execute(self):\n",
        "    clear_output()\n",
        "    print(self.model_id)\n",
        "    print()\n",
        "\n",
        "    self.create()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    colab_stats_max = [0,0,0]\n",
        "    for id_question in range(len(self.list_questions)):\n",
        "      clear_output()\n",
        "      print(model_name)\n",
        "      print()\n",
        "\n",
        "      turns = self.list_questions[id_question]['turns']\n",
        "      user_input_list = []\n",
        "      for turn in turns:\n",
        "\n",
        "        sugestions = self.get_sugestions(turn)\n",
        "        user_input_list.append({\"role\":\"user\",\"content\":turn + \"\\n\" + sugestions})\n",
        "        print(\"User: {}\".format(turn))\n",
        "        response = self.get_response(user_input_list)\n",
        "        print(\"Recbot: {}\".format(response))\n",
        "        user_input_list.append({\"role\":\"assistant\",\"content\":response})\n",
        "\n",
        "      rating = self.rating_conversation(user_input_list)\n",
        "      print('\\nAvaliação {}: {}\\n'.format(str(id_question + 1),rating))\n",
        "      write_cell_sheet(str(rating), get_column_letter(id_question + 6) + str(self.row_sheet_num))\n",
        "      colab_stats_max = np.maximum(colab_stats_max, self.get_colab_stats())\n",
        "      time.sleep(5)\n",
        "\n",
        "    write_cell_sheet(\"Disk: {} GiB, CPU: {} GiB, GPU: {} MiB\".format(colab_stats_max[0], colab_stats_max[1], colab_stats_max[2]), 'C' + str(self.row_sheet_num))\n",
        "    write_cell_sheet('success', 'D' + str(self.row_sheet_num))\n",
        "\n",
        "  def clean(self):\n",
        "    models_dir = '/root/.cache/huggingface/hub'\n",
        "    list_models_names = os.listdir(models_dir)\n",
        "    for model_name in list_models_names:\n",
        "      if \".txt\" not in model_name:\n",
        "        shutil.rmtree(models_dir + '/' + model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJt4bzTJ73QB"
      },
      "outputs": [],
      "source": [
        "class gguf_model(standard_model_eval):\n",
        "  def __init__(self, model_id, row_sheet_num, list_questions, llm_judge, retriever):\n",
        "    super().__init__(model_id, row_sheet_num, list_questions, llm_judge, retriever)\n",
        "\n",
        "  def create(self):\n",
        "    l_index = self.model_id.find('/') + 1\n",
        "    n_index = self.model_id[l_index:].find('/')\n",
        "    f_index = l_index + n_index\n",
        "    model_language = self.model_id[:f_index]\n",
        "    model_file = self.model_id[f_index + 1:]\n",
        "    self.model = Llama.from_pretrained(repo_id = model_language, filename = model_file, verbose = False, n_batch = 256, n_ctx = 2048, n_gpu_layers = -1)\n",
        "\n",
        "  def generate_text(self, prompt):\n",
        "    outputs = self.model(prompt, max_tokens = 300, stop = self.stop_strings, temperature=0, echo = False)\n",
        "    text = outputs['choices'][0]['text']\n",
        "    return text\n",
        "\n",
        "class second_state_model(standard_model_eval):\n",
        "  def __init__(self, model_id, row_sheet_num, list_questions, llm_judge, retriever):\n",
        "    super().__init__(model_id, row_sheet_num, list_questions, llm_judge, retriever, flag_chat = 2)\n",
        "\n",
        "  def create(self):\n",
        "    !{'curl -LO https://github.com/second-state/llamaedge/releases/latest/download/llama-api-server.wasm'}\n",
        "\n",
        "    def start_server():\n",
        "      l_index = self.model_id.find('/') + 1\n",
        "      n_index = self.model_id[l_index:].find('/')\n",
        "      f_index = l_index + n_index\n",
        "      model_language = self.model_id[:f_index]\n",
        "      model_file = self.model_id[f_index + 1:]\n",
        "      c1 = f'curl -LO https://huggingface.co/{model_language}/resolve/main/{model_file}'\n",
        "      c2 = f'wasmedge --dir .:. --nn-preload default:GGML:AUTO:{model_file} llama-api-server.wasm --prompt-template mistral-instruct --socket-addr 0.0.0.0:8000 --temp 0.0 --n-predict 300 --ctx-size 2048'\n",
        "      !{c1}\n",
        "      !{c2}\n",
        "\n",
        "    service = Process(target=start_server)\n",
        "    service.start()\n",
        "    while True:\n",
        "      test_server =  !curl http://localhost:8000\n",
        "      if test_server == []:\n",
        "        break\n",
        "      time.sleep(5)\n",
        "      pass\n",
        "\n",
        "  def generate_text(self, prompt):\n",
        "    r = 'curl -X POST http://localhost:8000/v1/chat/completions -H \\'accept:application/json\\' -H \\'Content-Type: application/json\\' -d \\'{\"messages\":' + prompt + '}\\''\n",
        "    response = !{r}\n",
        "    json_response = json.loads(response[0])\n",
        "    text = json_response['choices'][0][\"message\"][\"content\"]\n",
        "    return text\n",
        "\n",
        "  def clean(self):\n",
        "    current_process = psutil.Process()\n",
        "    for child in current_process.children(recursive=True):\n",
        "      child.terminate()\n",
        "    folder_path = '/content'\n",
        "    folder_content_paths = os.listdir(folder_path)\n",
        "    for content_path in folder_content_paths:\n",
        "      absolute_path = os.path.join(folder_path, content_path)\n",
        "      if os.path.isfile(absolute_path):\n",
        "        os.remove(absolute_path)\n",
        "      else:\n",
        "        shutil.rmtree(absolute_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKmCkJwENmfE",
        "outputId": "844295dc-95e7-43d8-8d83-5328830ece84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "meta-llama/Meta-Llama-3.1-8B-Instruct\n",
            "bigscience/bloomz-560m\n",
            "bigscience/bloom-1b7\n",
            "bigscience/bloom-560m\n",
            "bigscience/bloom-7b1\n",
            "meta-llama/Meta-Llama-3.1-8B\n",
            "mistralai/Mistral-Nemo-Instruct-2407\n",
            "lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF\n",
            "meta-llama/Meta-Llama-3.1-70B-Instruct\n",
            "stabilityai/stablelm-2-1_6b\n",
            "meta-llama/Meta-Llama-3.1-405B\n",
            "tiiuae/falcon-11B\n",
            "second-state/Mistral-Nemo-Instruct-2407-GGUF\n",
            "bigscience/bloom-1b1\n",
            "CohereForAI/aya-23-8B\n",
            "BioMistral/BioMistral-7B\n",
            "bullerwins/Meta-Llama-3.1-8B-Instruct-GGUF\n",
            "meta-llama/Meta-Llama-3.1-405B-Instruct-FP8\n",
            "CohereForAI/c4ai-command-r-v01-4bit\n",
            "ai-forever/mGPT\n",
            "alpindale/c4ai-command-r-plus-GPTQ\n",
            "meta-llama/Meta-Llama-3.1-70B\n",
            "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4\n",
            "MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF\n",
            "meta-llama/Meta-Llama-3.1-405B-Instruct\n",
            "facebook/xglm-564M\n",
            "bigscience/bloomz-7b1\n",
            "mistralai/Mistral-Nemo-Base-2407\n",
            "lmstudio-community/Meta-Llama-3.1-70B-Instruct-GGUF\n",
            "bigscience/bloom-3b\n",
            "bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\n",
            "hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\n",
            "bartowski/Mistral-Nemo-Instruct-2407-GGUF\n",
            "bigscience/bloom\n",
            "bartowski/aya-23-8B-GGUF\n",
            "bigscience/bloom-560m-intermediate\n",
            "mistralai/Mistral-Large-Instruct-2407\n",
            "bigscience/bloomz-3b\n",
            "bullerwins/Meta-Llama-3.1-70B-Instruct-GGUF\n",
            "legraphista/Mistral-Large-Instruct-2407-IMat-GGUF\n",
            "legraphista/Meta-Llama-3.1-70B-Instruct-IMat-GGUF\n",
            "meta-llama/Meta-Llama-3.1-405B-FP8\n",
            "legraphista/Meta-Llama-3.1-8B-Instruct-IMat-GGUF\n",
            "QuantFactory/Meta-Llama-3.1-8B-Instruct-GGUF\n",
            "pierreguillou/gpt2-small-portuguese\n",
            "bigscience/bloomz-1b7\n"
          ]
        }
      ],
      "source": [
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "\n",
        "list_models_name = get_list_models_name()\n",
        "list_questions = get_list_questions()\n",
        "retriever_model = get_retriever()\n",
        "gpt_judge = client = AzureOpenAI(azure_endpoint = \"https://ai-wallacesantana843ai809815112369.openai.azure.com/\", api_key= userdata.get('AZURE_KEY'), api_version=\"2024-02-15-preview\")"
      ]
    }
  ]
}